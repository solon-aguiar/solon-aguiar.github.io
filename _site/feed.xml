<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-10-09T16:58:35-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Solon Aguiar</title><subtitle>This is Solon's personal website. His personal ramblings and thoughts about software, travelling, life, philosophy, psychology &amp; beyond. A way to share and learn. Opinions are personal.</subtitle><entry><title type="html">The tech interview process</title><link href="http://localhost:4000/tech/interview/2018/10/21/tech-interview.html" rel="alternate" type="text/html" title="The tech interview process" /><published>2018-10-21T15:11:00-07:00</published><updated>2018-10-21T15:11:00-07:00</updated><id>http://localhost:4000/tech/interview/2018/10/21/tech-interview</id><content type="html" xml:base="http://localhost:4000/tech/interview/2018/10/21/tech-interview.html">&lt;p&gt;For better or for worse, I’ve been through a lot of different interview processes at tech companies to the point that I consider myself experienced in interviewing for jobs. It doesn’t mean that I am good at it, it just means that I understand the processes relatively well.&lt;/p&gt;

&lt;p&gt;I’ve interviewed at a wide range of tech companies: from Fortune 100 to start-ups and I’ve got to experience a plethora of different ideas on what is thought to be the best way to assess talent. I’ve seen well structured processes, some that try to evaluate different types of skills and, unfortunately, a lot that are very confusing and not inclusive. All of them have weaknesses. Some a lot more than others.&lt;/p&gt;

&lt;p&gt;A lot is said about the lack of diversity in tech. I believe that it is a huge problem and I sincerely believe that a lot of it has to do with the fact that tech hiring is broken in so many ways. I list some of what I think are the reasons for that below.&lt;/p&gt;

&lt;p&gt;Disclaimer #1: my track record is roughly one offer at every 3 companies I interview with - I am not counting the ones I applied and was rejected even before one interview - all of this, for my sadness, has showed me a wide share of rejection emails. I’ve applied for positions that required a lot of the experience that I have and for positions that I barely match any of requirements. I don’t know if this is a good of a bad track record, but that doesn’t invalidate my thoughts or observations. Also, they don’t come from any grudge I hold, and some of them are even from companies I’ve gotten offers for.&lt;/p&gt;

&lt;p&gt;Disclaimer #2: Even though some of the time I’ve disagreed with the final decision of a rejection, I have never believed that any rejection was for any reason other than my technical skills, such as bias or prejudice. I believe that everyone I interviewed with has been solely focused on my demonstration (or lack) of abilities to do the job.&lt;/p&gt;

&lt;p&gt;Disclaimer #3: It is important to notice that in some cases I am generalizing. Not all the situations happen like that all the time, but at least one of them are common in each process I’ve been through.&lt;/p&gt;

&lt;p&gt;I believe it is important to discuss this. Discussing problems is the only way to search for solutions. It allows us to recognize different perspectives, see gaps that we didn’t see before and open our mind.&lt;/p&gt;

&lt;p&gt;–&lt;/p&gt;

&lt;h1 id=&quot;and-the-common-tech-recruiting-problems-are&quot;&gt;And the common tech recruiting problems are…&lt;/h1&gt;

&lt;h3 id=&quot;the-shadow-recruiter&quot;&gt;The shadow-recruiter&lt;/h3&gt;

&lt;p&gt;A lot of the time, the first step in any process is a conversation with a recruiter. This person’s role in the process is to be the first layer of filtering and also be your point of contact throughout your time interviewing.&lt;/p&gt;

&lt;p&gt;Most of the time, the conversation looms around your interests, your experience and what you’re looking for. It can also go through the dreaded salary expectations question. The recruiter will say a bunch of buzz words while waiting to hear others back (to see if there is a “match in expectations”).&lt;/p&gt;

&lt;p&gt;For companies, it is a matter of saving precious engineering time with candidates that aren’t worth it. For candidates, it is mostly a waste of time except in the rare cases the recruiter has some information that isn’t on the company’s website (which is rare).&lt;/p&gt;

&lt;p&gt;I call this problem “the shadow recruiter” because the during this first conversation, the recruiter doesn’t have any useful information for the candidate. And, after the other interviews start, they just vanish. Many many times, in many different companies, the recruiters have turned out to be unresponsive with updates, feedback or any other useful information. They just don’t provide you with information. You’re left in the dark until you reach out. It is a common pattern that I constantly have to reach out one or two times before getting any update. This is terrible and frustrating candidate experience.&lt;/p&gt;

&lt;h3 id=&quot;the-all-in-be-perfect-approach&quot;&gt;The all-in-be-perfect-approach&lt;/h3&gt;

&lt;p&gt;Ah, the “no false positives”… That is surely one of the most frustrating things in the tech interview world. Companies devise a process to assess candidates in many areas and in the slightest sign of doubt, they just let the candidate pass. It doesn’t matter if the sign was significant, occurred only once or in all interviews. A single slip from the candidate after 6 or 7 hours of interviewing is enough data for a company to decide that they aren’t fit for the job.&lt;/p&gt;

&lt;p&gt;Well, that just makes sense, right? All of my current employees can be grilled by people that they don’t know for 5 hours straight and not make a single mistake. Why can’t you, dear candidate? Making a mistake in this hour-long interview shows me that you’ll do the same while you’re doing your job here, which will make me waste my time and money. I’ll just disregard the other 6 hours you spent with us. Therefore, thank you for your time, good bye. Maybe try again next year?&lt;/p&gt;

&lt;p&gt;I think that people who devise these strategies or believe this have not gone through the very own process they designed. Interviewing is stressful and hard for almost everyone. Your career, and a lot of times your future, is on the line. You are going to make mistakes. And that is normal. Everyone makes mistakes while doing their job. I do that all the time today. But why is it so harmful during an interview? Why one bad interview in 7 is enough data to show that you’ll underperform in the job? Is this really the case or are we just scared or standing by our thoughts?&lt;/p&gt;

&lt;h3 id=&quot;the-non-diverse-panel&quot;&gt;The non-diverse-panel&lt;/h3&gt;

&lt;p&gt;Everyone wants to increase diversity, yet no one strives to make it easy to do so. Even when they have a shot at it, they fail. It is so hard, not to mention harmful, to go through a process and only see people that lookalike (and most of the time different from you). Interviewers will have unconscious biases that can dictate what they think about and judge of you. That can undermine your candidacy and chances of success.&lt;/p&gt;

&lt;p&gt;In order to promote diversity and inclusion, companies should promote D&amp;amp;I in their interview panel. They should have panels that are diverse and bring different perspectives and ideas to the table. That will make the candidates feel welcomed by seeing diversity and, more importantly, reduce the effect of biases in evaluation.&lt;/p&gt;

&lt;h3 id=&quot;the-no-feedback-policy&quot;&gt;The no-feedback-policy&lt;/h3&gt;

&lt;p&gt;As a candidate that fails a process, there is very seldom room to know what you were missing. A lot of the time, companies provide no feedback on their evaluation to you. I understand that for legal reasons they can’t just share what they think and I’m not preaching for that. I do believe, though, that if we don’t provide any feedback we’re not making any progress towards giving more people chances. If people don’t know specifically what they have to get better at, how can they have a chance at it?&lt;/p&gt;

&lt;p&gt;I admit that not everyone is keen of receiving evaluation. I think, though, that any time someone reaches out to get feedback, companies should promptly and respectfully provide candidates with useful information that they can use to address their gaps. I see that as a responsibility of the companies. After all, candidates have invested so much of their time in going through the process that the minimum the companies can do is provide them with some thoughts. I believe that there is a way of doing that without being liable for lawsuits and not being generic as “get better at algorithms and study data structures”.&lt;/p&gt;

&lt;h3 id=&quot;the-i-do-not-need-to-respond-anymore&quot;&gt;The I-do-not-need-to-respond-anymore&lt;/h3&gt;

&lt;p&gt;Another very frustrating pattern is being left in the dark after a final decision is made. Any time I get a decision for good or bad I send an email back with comments and or questions and most of the time I never hear back in the case of rejection. It is as if I have a contagious disease and people at that company don’t want to talk to me any more after they realized I am not perfect for their position. They don’t want to “waste” time with me anymore.&lt;/p&gt;

&lt;p&gt;The funny thing is that any time before the final decision, they are all happy and cheerful in answering. Things just suddenly change when they decide that it’s a no go. Now, you’re not worth their attention anymore. This is really hurting.&lt;/p&gt;

&lt;p&gt;We’re all humans. We are bond to contact people and to have feelings. Especially when we invest so much time in a process. It is really easy to just hide behind an email inbox, but we don’t have to be like that. Be human too and at least acknowledge the other person’s communication, even if it is just to say thank you and move on. Don’t leave them hanging. That hurts and will surely impact their view of your company for the future.&lt;/p&gt;

&lt;p&gt;–&lt;/p&gt;

&lt;h1 id=&quot;and-what-is-the-perfect-process&quot;&gt;And what is the perfect process?&lt;/h1&gt;

&lt;p&gt;There likely isn’t a perfect process. Evaluating people is hard and not a precise science. From the moment you have employees evaluating candidates, you’re subject to biases and flaws. It is our nature. And the process that is thought to address this a lot of the time doesn’t. More importantly, processes aren’t perfect. They are subject to failure too and we shouldn’t shy away from noticing and voicing that. We shouldn’t hide behind the flag of a process as way to protect ourselves from making a hard or disagreeable decision. Trusting the process is the easy thing to do, but can often also be the wrong one.&lt;/p&gt;

&lt;p&gt;I believe that we can get better. Addressing the issues I listed above will surely make processes more humane and somewhat just. Do however you want to do: algorithmical questions, small code problems, just talking or a mix of all of that. All of these ideas of measuring one’s knowledge are subject to the flaws I listed above. Being aware of that and constantly measuring it will help you make sure you’re on track.&lt;/p&gt;</content><author><name></name></author><summary type="html">The thoughts behind many interview processes I've been through</summary></entry><entry><title type="html">Exponential Back-off</title><link href="http://localhost:4000/design-pattern/backend/2018/09/17/exponential-backoff.html" rel="alternate" type="text/html" title="Exponential Back-off" /><published>2018-09-17T16:36:00-07:00</published><updated>2018-09-17T16:36:00-07:00</updated><id>http://localhost:4000/design-pattern/backend/2018/09/17/exponential-backoff</id><content type="html" xml:base="http://localhost:4000/design-pattern/backend/2018/09/17/exponential-backoff.html">&lt;p&gt;Following my post on &lt;a href=&quot;https://solon-aguiar.github.io/design-pattern/backend/2018/09/06/circuit-breaker-pattern.html&quot;&gt;Circuit Breaker&lt;/a&gt; I will now cover &lt;em&gt;Exponential Back-off&lt;/em&gt;. This pattern can be used along with &lt;em&gt;Circuit Breaker&lt;/em&gt;, but can also be used without it. In general, for any distributed system - especially one that operates at high scale, retrying failed remote requests with an exponential back-off is a good idea.&lt;/p&gt;

&lt;h1 id=&quot;the-problem&quot;&gt;The problem&lt;/h1&gt;
&lt;p&gt;Let’s elaborate on the same &lt;strong&gt;AccountSystem&lt;/strong&gt; example from the last post. As we saw, that system requests data from the &lt;strong&gt;UserAccountsDB&lt;/strong&gt;, which is a hard dependency as without the data, the &lt;strong&gt;AccountSystem&lt;/strong&gt; cannot do much computation.&lt;/p&gt;

&lt;p&gt;Since we’re working in a distributed system, we know that the remote request to the database can fail for a multitude of reasons: network blip, data loss, timeout while opening/re-establishing connection, process re-started, full request queue etc. To make our system more resilient and avoid throwing errors to our callers every time something happens we decide to retry a request every time it fails (we’ll discuss the downside of this later). This will decrease our error response rate and make everyone happier.&lt;/p&gt;

&lt;h1 id=&quot;the-solution&quot;&gt;The solution&lt;/h1&gt;

&lt;p&gt;To implement retries, we change of our main application logic to look something like this (pseudocode for a hypothetical 3 retries implementation):&lt;/p&gt;

&lt;div class=&quot;language-ruby highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;database&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;UserAccountsDB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;

&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;times&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;read_user_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;user_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Nil&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We make the change, test and see that this code works. It retries each failed request up to three times. Great, we solved the problem… but created another. The problem with this approach is that it doesn’t give time for whatever was causing the request to fail to be fixed - either by itself of via external interference. In other words, it doesn’t “give” time for the system to recover. It simply keeps retrying until it succeeds or until it reaches the end of retries as we don’t want to keep retrying forever.&lt;/p&gt;

&lt;p&gt;What can we do to tackle this?&lt;/p&gt;

&lt;p&gt;We can just wait a little bit! Let’s add a little wait (let’s say 200 ms) after each failure to give the system some time to recover from whatever is causing it to fail. Our code becomes this:&lt;/p&gt;

&lt;div class=&quot;language-ruby highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;database&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;UserAccountsDB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;

&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;times&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;read_user_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;user_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Nil&lt;/span&gt;
   &lt;span class=&quot;nb&quot;&gt;system&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now every time a request fails we wait a little bit (in this case I chose 200 ms arbitrarily) and then retry. The advantage of this is that, empirically - not mathematically, it is more likely that subsequent requests will succeed after a failure because we waited a little bit.&lt;/p&gt;

&lt;p&gt;However, this implementation also has a problem. The problem is a little bit more subtle and normally only surfaces at large scale systems under very specific conditions (that aren’t that rare as I’ll tell you later in this post).&lt;/p&gt;

&lt;h2 id=&quot;at-scale&quot;&gt;At scale&lt;/h2&gt;

&lt;p&gt;Let’s imagine that our &lt;strong&gt;AccountSystem&lt;/strong&gt; is running in production. Since we have a very successful company, we get a lot of traffic and have to scale it to 600 hosts to accommodate all the traffic. All of these hosts connect to the &lt;strong&gt;UserAccountsDB&lt;/strong&gt; to read user data. Now let’s imagine that for some unfortunate reason, the database goes down. What will happen to our &lt;strong&gt;AccountSystem&lt;/strong&gt;? Since earlier we decided to make our system resilient via retries, it won’t just start throwing errors at the callers. It will first retry failed requests. This is great until it isn’t.&lt;/p&gt;

&lt;p&gt;All of our 600 hosts have received failures from the database when it went down, so all of these hosts have started to put their threads to sleep in order to retry those requests later. Until that point, nothing bad, everything working as we designed. But what happens when the &lt;strong&gt;UserAccountsDB&lt;/strong&gt; comes back up? All of the “pending” requests (which can be a lot more than 600 assuming a system that handles more than one request at a time) will be retried in a very short span of time - possibly at the same time!&lt;/p&gt;

&lt;p&gt;Since the failures started happening at around the same moment, the instances of the &lt;strong&gt;AccountSystem&lt;/strong&gt; decided to “sleep” on those in hopes that they would be fixed. When the database came back up and the system “woke up”, it started to retry everything that had been pending. Since the hosts most likely put the threads to sleep around the same time, all requests were retried simultaneously (or within a few seconds apart). This is potentially catastrophic for the database.&lt;/p&gt;

&lt;p&gt;Most of the time, the databases aren’t scaled or ready to receive this massive load of requests (and for legitimate reasons!). Normally, databases don’t have to match the scale of the systems that lives in front of them because they don’t take all the load that those receive (obviously this assumption doesn’t hold true in all the cases - it is a generalization of my part). Therefore, when all the requests are retried, the database can be overwhelmed and either perform really badly or even go down (often aggravating the problem itself). It can become a disastrous situation that requires a lot of manual work to be addressed as it keeps breaking itself.&lt;/p&gt;

&lt;p&gt;The key problem here are the simultaneously retried requests. To address this issue, the &lt;em&gt;Exponential Backoff&lt;/em&gt; comes into play. The idea is that each time a request fails, it “tells” us something about the environment. Basically, the more times the same request fails, the more we wait before the next retry, as previous errors indicate that something is not working well. Our pseudocode becomes something like this:&lt;/p&gt;

&lt;div class=&quot;language-ruby highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;database&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;UserAccountsDB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;

&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;times&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;read_user_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;user_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Nil&lt;/span&gt;
   &lt;span class=&quot;nb&quot;&gt;system&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exponential_backoff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The implementation of the &lt;code class=&quot;highlighter-rouge&quot;&gt;exponential_backoff&lt;/code&gt; can vary on the application, but in general it can be something like this:&lt;/p&gt;

&lt;div class=&quot;language-ruby highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;exponential_backoff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The idea is that we have a base value (in this case 100) and that we randomly wait a few extra milliseconds after each failure (hence the power operation). The random factor is used to avoid that all different requests retry at the same time (situation I described above). Different applications have different ways of implementing the back-off calculation itself (some deal with the base values differently, want to guarantee randomness etc.), but that is out of the scope of this post.&lt;/p&gt;

&lt;p&gt;With this implementation, when the database starts failing and the requests go to sleep, they will be awaken at different times, which will avoid hammering the database with a big number of requests. Some requests can and will be retried simultaneously, but due to the randomness factor the number won’t be as high as before. This will allow our system to gracefully come back up.&lt;/p&gt;

&lt;h1 id=&quot;the-downside&quot;&gt;The downside&lt;/h1&gt;

&lt;p&gt;Like everything in life, &lt;em&gt;Exponential Backoff&lt;/em&gt; has a downside. A few of them are worth mentioning:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Each request that has a failure can take potentially longer to complete: the system might spend unnecessary time waiting before retrying a request. Let’s look at an example: say that we started retry #2 for 3 seconds at t0. Imagine that the database came back at t1. Since we’re waiting until t3, it won’t be until that time that the system will be retry and succeeded on the pending request. As you can see, the system wasted 2 seconds.&lt;/li&gt;
  &lt;li&gt;The code gets more complex: Now every retry requires a calculation before it goes to sleep. Our code got a little bit more complex and can be hard to test if you don’t inject the dependencies correctly.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It is a trade-off. Sometimes the resilience is worth it sometimes it isn’t. It depends on your system.&lt;/p&gt;

&lt;h1 id=&quot;in-practice&quot;&gt;In practice&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Exponential Backoff&lt;/em&gt; is a simple solution to a common problem in any computational system. However, you can’t take its usage for granted. In my industry experience I’ve seen real big production issues in Fortune 500 companies that could have been avoided just by using a simple back-off strategy. Each of these failures cost a lot of money and reputation for these companies.&lt;/p&gt;

&lt;h1 id=&quot;coming-up&quot;&gt;Coming up&lt;/h1&gt;
&lt;p&gt;I’ll continue on this series of patterns for system design. Next time I’ll talk about using queues for buffering and processing.&lt;/p&gt;</content><author><name></name></author><summary type="html">The Exponential Back-off is pattern for retrying failed computations.</summary></entry><entry><title type="html">Circuit Breaker for higher availability and reliability</title><link href="http://localhost:4000/design-pattern/backend/2018/09/06/circuit-breaker-pattern.html" rel="alternate" type="text/html" title="Circuit Breaker for higher availability and reliability" /><published>2018-09-06T14:36:00-07:00</published><updated>2018-09-06T14:36:00-07:00</updated><id>http://localhost:4000/design-pattern/backend/2018/09/06/circuit-breaker-pattern</id><content type="html" xml:base="http://localhost:4000/design-pattern/backend/2018/09/06/circuit-breaker-pattern.html">&lt;p&gt;The &lt;em&gt;Circuit Breaker&lt;/em&gt; pattern is a very useful pattern to improve reliability and availability of a back-end service or micro-service. The idea behind it is actually quite simple: if a system depends on another system (be this latter one a database, remote or distributed file system, back-end etc - any type of remote system that requires a connection), it can keep track of the state of that connection and take immediate action when that dependency is down or not functioning properly in order to alleviate resources.&lt;/p&gt;

&lt;p&gt;An example will make it more clear. Let’s say that the &lt;strong&gt;AccountSystem&lt;/strong&gt; makes requests to the &lt;strong&gt;UserAccountsDB&lt;/strong&gt; in order to read all user data and respond with the user’s subscriptions. Clearly, the database is a requirement for the account system to work properly. If this database is not accessible then there’s not much the &lt;strong&gt;AccountSystem&lt;/strong&gt; can do with every request. Therefore, it is a good design pattern for the &lt;strong&gt;AccountSystem&lt;/strong&gt; to keep track of the state of that connection and make sure it only tries to fetch user data when it knows it can get the valuable information. In cases when it knows that the connection is not working, it can just reply an error to the caller. See the diagrams below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cb_connection_ok.png&quot; alt=&quot;Connection Ok&quot; /&gt;
&lt;img src=&quot;/assets/images/cb_connection_not_ok.png&quot; alt=&quot;Connection Not Ok&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This short circuited response allows the system as a whole to free up resources. If the &lt;strong&gt;AccountSystem&lt;/strong&gt; still tried to fetch the user’s data, it would use more of its own and the caller’s CPU time as well as network bandwidth. By not making the call it knows that can’t be fulfilled, it saves cpu clocks, bandwidth and time. A client of this service can quickly act on that error response however it deem necessary.&lt;/p&gt;

&lt;p&gt;Another benefit of this approach is that whenever the database system does become available, then it won’t likely be overflowed with outstanding requests from the &lt;strong&gt;AccountSystem&lt;/strong&gt;, thus reducing the availability risk.&lt;/p&gt;

&lt;p&gt;While the idea behind the &lt;em&gt;Circuit Breaker&lt;/em&gt; pattern is simple, it’s implementation can be a bit tricky (even though you can find some code on Wikipedia etc.). The thing is that maintaining the state of this connection (or many others in case your system depends on more than 1 remote system) is not trivial. You have to be careful with the transitions between those states. A few interesting questions to ask yourself:
When do you mark a connection as down? It is not a really good idea to do at every error as those can happen due to network and software glitches etc.
What is the time period to consider a connection down? X errors in Y seconds is enough? What are acceptable values of X and Y that will give the benefit of the short circuit and not cause too many false positives?
Once a connection is marked as down, how often do you check for its availability again, so that you can re-open the circuit? This is critical to be able to fully respond to requests again. When thinking about this one, it is specifically important to think how you keep retrying. A good alternative is using exponential backoffs, which will be the topic of my next post.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;Circuit Breaker&lt;/em&gt; pattern is very useful in the case of hard dependencies on other systems, but not the only case where it can be used. It can also be successfully applied in cases when the dependent system is not critical (such as a system that gives you updated weather forecasts in your exercise app). While up-to-date data is desired in all scenarios, some systems can tolerate stale data (even with a warning to the user) and continue to function properly. The &lt;em&gt;Circuit Breaker&lt;/em&gt;, in this case, will help free up resources, as said before, and reduce request time.&lt;/p&gt;

&lt;p&gt;Note that using the pattern will incur more complexity and possibly add some overhead to each (or some - depending on how often you check it) operations. It can also cause the whole system to take longer to “realize” the dependency is back (depending on how you check for the connection again).&lt;/p&gt;

&lt;p&gt;As I said, using the &lt;em&gt;Circuit Breaker&lt;/em&gt; improves reliability and availability. It does so because it makes your system more resilient to errors. In most cases the benefits outweigh the drawbacks and make it a good idea to use it.&lt;/p&gt;

&lt;p&gt;While using a &lt;em&gt;Circuit Breaker&lt;/em&gt;, it is generally a good idea to use an &lt;em&gt;Exponential Backoff&lt;/em&gt; for testing the health of the dependent connections. Exponential Backoffs will be the topic of my next blog post.&lt;/p&gt;

&lt;p&gt;Stay tuned!&lt;/p&gt;</content><author><name></name></author><summary type="html">The Circuit Breaker pattern is a very useful pattern to improve reliability and availability of a back-end service or micro-service.</summary></entry><entry><title type="html">Welcome to my new website!</title><link href="http://localhost:4000/about/blog/2018/09/05/welcome-to-my-website.html" rel="alternate" type="text/html" title="Welcome to my new website!" /><published>2018-09-05T15:04:44-07:00</published><updated>2018-09-05T15:04:44-07:00</updated><id>http://localhost:4000/about/blog/2018/09/05/welcome-to-my-website</id><content type="html" xml:base="http://localhost:4000/about/blog/2018/09/05/welcome-to-my-website.html">&lt;p&gt;This is my new website. It is built on jekyll and hosted on github pages. It is a simple yet powerful and extensible set up.&lt;/p&gt;

&lt;p&gt;I plan to use it as a way to share some of my personal thoughts and whatever interesting topics (in my field of work or not) I feel good about writing.
You shouldn’t expect scheduled updates. Even though I plan to be dilligent about writing, I won’t just write for the sake of writing, so updates will come as I find interesting things to write about.&lt;/p&gt;

&lt;p&gt;The first series I plan on writing about is about patterns for reliable backend systems (or microservices). Stay tuned!&lt;/p&gt;</content><author><name></name></author><summary type="html">This is my new website. It is built on jekyll and hosted on github pages. It is a simple yet powerful and extensible set up.</summary></entry></feed>
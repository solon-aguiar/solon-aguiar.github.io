<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.5.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-09-17T22:01:18-04:00</updated><id>http://localhost:4000/</id><title type="html">Solon Aguiar</title><subtitle>This is Solon's personal website. His personal ramblings and thoughts about software, travelling, life, philosophy, psychology &amp; beyond. A way to share and learn. Opinions are personal.</subtitle><entry><title type="html">Exponential Back-off</title><link href="http://localhost:4000/design-pattern/backend/2018/09/17/exponential-backoff.html" rel="alternate" type="text/html" title="Exponential Back-off" /><published>2018-09-17T19:36:00-04:00</published><updated>2018-09-17T19:36:00-04:00</updated><id>http://localhost:4000/design-pattern/backend/2018/09/17/exponential-backoff</id><content type="html" xml:base="http://localhost:4000/design-pattern/backend/2018/09/17/exponential-backoff.html">&lt;p&gt;Following my post on &lt;a href=&quot;https://solon-aguiar.github.io/design-pattern/backend/2018/09/06/circuit-breaker-pattern.html&quot;&gt;Circuit Breaker&lt;/a&gt; I will now cover &lt;em&gt;Exponential Back-off&lt;/em&gt;. This pattern can be used along with &lt;em&gt;Circuit Breaker&lt;/em&gt;, but can also be used without it. In general, for any distributed system - especially one that operates at high scale, retrying failed remote requests with an exponential back-off is a good idea.&lt;/p&gt;

&lt;h1 id=&quot;the-problem&quot;&gt;The problem&lt;/h1&gt;
&lt;p&gt;Let’s elaborate on the same &lt;strong&gt;AccountSystem&lt;/strong&gt; example from the last post. As we saw, that system requests data from the &lt;strong&gt;UserAccountsDB&lt;/strong&gt;, which is a hard dependency as without the data, the &lt;strong&gt;AccountSystem&lt;/strong&gt; cannot do much computation.&lt;/p&gt;

&lt;p&gt;Since we’re working in a distributed system, we know that the remote request to the database can fail for a multitude of reasons: network blip, data loss, timeout while opening/re-establishing connection, process re-started, full request queue etc. To make our system more resilient and avoid throwing errors to our callers every time something happens we decide to retry a request every time it fails (we’ll discuss the downside of this later). This will decrease our error response rate and make everyone happier.&lt;/p&gt;

&lt;h1 id=&quot;the-solution&quot;&gt;The solution&lt;/h1&gt;

&lt;p&gt;To implement retries, we change of our main application logic to look something like this (pseudocode for a hypothetical 3 retries implementation):&lt;/p&gt;

&lt;div class=&quot;language-ruby highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;database&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;UserAccountsDB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;

&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;times&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;read_user_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;user_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Nil&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We make the change, test and see that this code works. It retries each failed request up to three times. Great, we solved the problem… but created another. The problem with this approach is that it doesn’t give time for whatever was causing the request to fail to be fixed - either by itself of via external interference. In other words, it doesn’t “give” time for the system to recover. It simply keeps retrying until it succeeds or until it reaches the end of retries as we don’t want to keep retrying forever.&lt;/p&gt;

&lt;p&gt;What can we do to tackle this?&lt;/p&gt;

&lt;p&gt;We can just wait a little bit! Let’s add a little wait (let’s say 200 ms) after each failure to give the system some time to recover from whatever is causing it to fail. Our code becomes this:&lt;/p&gt;

&lt;div class=&quot;language-ruby highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;database&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;UserAccountsDB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;

&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;times&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;read_user_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;user_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Nil&lt;/span&gt;
   &lt;span class=&quot;nb&quot;&gt;system&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now every time a request fails we wait a little bit (in this case I chose 200 ms arbitrarily) and then retry. The advantage of this is that, empirically - not mathematically, it is more likely that subsequent requests will succeed after a failure because we waited a little bit.&lt;/p&gt;

&lt;p&gt;However, this implementation also has a problem. The problem is a little bit more subtle and normally only surfaces at large scale systems under very specific conditions (that aren’t that rare as I’ll tell you later in this post).&lt;/p&gt;

&lt;h2 id=&quot;at-scale&quot;&gt;At scale&lt;/h2&gt;

&lt;p&gt;Let’s imagine that our &lt;strong&gt;AccountSystem&lt;/strong&gt; is running in production. Since we have a very successful company, we get a lot of traffic and have to scale it to 600 hosts to accommodate all the traffic. All of these hosts connect to the &lt;strong&gt;UserAccountsDB&lt;/strong&gt; to read user data. Now let’s imagine that for some unfortunate reason, the database goes down. What will happen to our &lt;strong&gt;AccountSystem&lt;/strong&gt;? Since earlier we decided to make our system resilient via retries, it won’t just start throwing errors at the callers. It will first retry failed requests. This is great until it isn’t.&lt;/p&gt;

&lt;p&gt;All of our 600 hosts have received failures from the database when it went down, so all of these hosts have started to put their threads to sleep in order to retry those requests later. Until that point, nothing bad, everything working as we designed. But what happens when the &lt;strong&gt;UserAccountsDB&lt;/strong&gt; comes back up? All of the “pending” requests (which can be a lot more than 600 assuming a system that handles more than one request at a time) will be retried in a very short span of time - possibly at the same time!&lt;/p&gt;

&lt;p&gt;Since the failures started happening at around the same moment, the instances of the &lt;strong&gt;AccountSystem&lt;/strong&gt; decided to “sleep” on those in hopes that they would be fixed. When the database came back up and the system “woke up”, it started to retry everything that had been pending. Since the hosts most likely put the threads to sleep around the same time, all requests were retried simultaneously (or within a few seconds apart). This is potentially catastrophic for the database.&lt;/p&gt;

&lt;p&gt;Most of the time, the databases aren’t scaled or ready to receive this massive load of requests (and for legitimate reasons!). Normally, databases don’t have to match the scale of the systems that lives in front of them because they don’t take all the load that those receive (obviously this assumption doesn’t hold true in all the cases - it is a generalization of my part). Therefore, when all the requests are retried, the database can be overwhelmed and either perform really badly or even go down (often aggravating the problem itself). It can become a disastrous situation that requires a lot of manual work to be addressed as it keeps breaking itself.&lt;/p&gt;

&lt;p&gt;The key problem here are the simultaneously retried requests. To address this issue, the &lt;em&gt;Exponential Backoff&lt;/em&gt; comes into play. The idea is that each time a request fails, it “tells” us something about the environment. Basically, the more times the same request fails, the more we wait before the next retry, as previous errors indicate that something is not working well. Our pseudocode becomes something like this:&lt;/p&gt;

&lt;div class=&quot;language-ruby highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;database&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;UserAccountsDB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;new&lt;/span&gt;

&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;times&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;read_user_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;user_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Nil&lt;/span&gt;
   &lt;span class=&quot;nb&quot;&gt;system&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exponential_backoff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The implementation of the &lt;code class=&quot;highlighter-rouge&quot;&gt;exponential_backoff&lt;/code&gt; can vary on the application, but in general it can be something like this:&lt;/p&gt;

&lt;div class=&quot;language-ruby highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;exponential_backoff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The idea is that we have a base value (in this case 100) and that we randomly wait a few extra milliseconds after each failure (hence the power operation). The random factor is used to avoid that all different requests retry at the same time (situation I described above). Different applications have different ways of implementing the back-off calculation itself (some deal with the base values differently, want to guarantee randomness etc.), but that is out of the scope of this post.&lt;/p&gt;

&lt;p&gt;With this implementation, when the database starts failing and the requests go to sleep, they will be awaken at different times, which will avoid hammering the database with a big number of requests. Some requests can and will be retried simultaneously, but due to the randomness factor the number won’t be as high as before. This will allow our system to gracefully come back up.&lt;/p&gt;

&lt;h1 id=&quot;the-downside&quot;&gt;The downside&lt;/h1&gt;

&lt;p&gt;Like everything in life, &lt;em&gt;Exponential Backoff&lt;/em&gt; has a downside. A few of them are worth mentioning:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Each request that has a failure can take potentially longer to complete: the system might spend unnecessary time waiting before retrying a request. Let’s look at an example: say that we started retry #2 for 3 seconds at t0. Imagine that the database came back at t1. Since we’re waiting until t3, it won’t be until that time that the system will be retry and succeeded on the pending request. As you can see, the system wasted 2 seconds.&lt;/li&gt;
  &lt;li&gt;The code gets more complex: Now every retry requires a calculation before it goes to sleep. Our code got a little bit more complex and can be hard to test if you don’t inject the dependencies correctly.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It is a trade-off. Sometimes the resilience is worth it sometimes it isn’t. It depends on your system.&lt;/p&gt;

&lt;h1 id=&quot;in-practice&quot;&gt;In practice&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Exponential Backoff&lt;/em&gt; is a simple solution to a common problem in any computational system. However, you can’t take its usage for granted. In my industry experience I’ve seen real big production issues in Fortune 500 companies that could have been avoided just by using a simple back-off strategy. Each of these failures cost a lot of money and reputation for these companies.&lt;/p&gt;

&lt;h1 id=&quot;coming-up&quot;&gt;Coming up&lt;/h1&gt;
&lt;p&gt;I’ll continue on this series of patterns for system design. Next time I’ll talk about using queues for buffering and processing.&lt;/p&gt;</content><author><name></name></author><summary type="html">The Exponential Back-off is pattern for retrying failed computations.</summary></entry><entry><title type="html">Circuit Breaker for higher availability and reliability</title><link href="http://localhost:4000/design-pattern/backend/2018/09/06/circuit-breaker-pattern.html" rel="alternate" type="text/html" title="Circuit Breaker for higher availability and reliability" /><published>2018-09-06T17:36:00-04:00</published><updated>2018-09-06T17:36:00-04:00</updated><id>http://localhost:4000/design-pattern/backend/2018/09/06/circuit-breaker-pattern</id><content type="html" xml:base="http://localhost:4000/design-pattern/backend/2018/09/06/circuit-breaker-pattern.html">&lt;p&gt;The &lt;em&gt;Circuit Breaker&lt;/em&gt; pattern is a very useful pattern to improve reliability and availability of a back-end service or micro-service. The idea behind it is actually quite simple: if a system depends on another system (be this latter one a database, remote or distributed file system, back-end etc - any type of remote system that requires a connection), it can keep track of the state of that connection and take immediate action when that dependency is down or not functioning properly in order to alleviate resources.&lt;/p&gt;

&lt;p&gt;An example will make it more clear. Let’s say that the &lt;strong&gt;AccountSystem&lt;/strong&gt; makes requests to the &lt;strong&gt;UserAccountsDB&lt;/strong&gt; in order to read all user data and respond with the user’s subscriptions. Clearly, the database is a requirement for the account system to work properly. If this database is not accessible then there’s not much the &lt;strong&gt;AccountSystem&lt;/strong&gt; can do with every request. Therefore, it is a good design pattern for the &lt;strong&gt;AccountSystem&lt;/strong&gt; to keep track of the state of that connection and make sure it only tries to fetch user data when it knows it can get the valuable information. In cases when it knows that the connection is not working, it can just reply an error to the caller. See the diagrams below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cb_connection_ok.png&quot; alt=&quot;Connection Ok&quot; /&gt;
&lt;img src=&quot;/assets/images/cb_connection_not_ok.png&quot; alt=&quot;Connection Not Ok&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This short circuited response allows the system as a whole to free up resources. If the &lt;strong&gt;AccountSystem&lt;/strong&gt; still tried to fetch the user’s data, it would use more of its own and the caller’s CPU time as well as network bandwidth. By not making the call it knows that can’t be fulfilled, it saves cpu clocks, bandwidth and time. A client of this service can quickly act on that error response however it deem necessary.&lt;/p&gt;

&lt;p&gt;Another benefit of this approach is that whenever the database system does become available, then it won’t likely be overflowed with outstanding requests from the &lt;strong&gt;AccountSystem&lt;/strong&gt;, thus reducing the availability risk.&lt;/p&gt;

&lt;p&gt;While the idea behind the &lt;em&gt;Circuit Breaker&lt;/em&gt; pattern is simple, it’s implementation can be a bit tricky (even though you can find some code on Wikipedia etc.). The thing is that maintaining the state of this connection (or many others in case your system depends on more than 1 remote system) is not trivial. You have to be careful with the transitions between those states. A few interesting questions to ask yourself:
When do you mark a connection as down? It is not a really good idea to do at every error as those can happen due to network and software glitches etc.
What is the time period to consider a connection down? X errors in Y seconds is enough? What are acceptable values of X and Y that will give the benefit of the short circuit and not cause too many false positives?
Once a connection is marked as down, how often do you check for its availability again, so that you can re-open the circuit? This is critical to be able to fully respond to requests again. When thinking about this one, it is specifically important to think how you keep retrying. A good alternative is using exponential backoffs, which will be the topic of my next post.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;Circuit Breaker&lt;/em&gt; pattern is very useful in the case of hard dependencies on other systems, but not the only case where it can be used. It can also be successfully applied in cases when the dependent system is not critical (such as a system that gives you updated weather forecasts in your exercise app). While up-to-date data is desired in all scenarios, some systems can tolerate stale data (even with a warning to the user) and continue to function properly. The &lt;em&gt;Circuit Breaker&lt;/em&gt;, in this case, will help free up resources, as said before, and reduce request time.&lt;/p&gt;

&lt;p&gt;Note that using the pattern will incur more complexity and possibly add some overhead to each (or some - depending on how often you check it) operations. It can also cause the whole system to take longer to “realize” the dependency is back (depending on how you check for the connection again).&lt;/p&gt;

&lt;p&gt;As I said, using the &lt;em&gt;Circuit Breaker&lt;/em&gt; improves reliability and availability. It does so because it makes your system more resilient to errors. In most cases the benefits outweigh the drawbacks and make it a good idea to use it.&lt;/p&gt;

&lt;p&gt;While using a &lt;em&gt;Circuit Breaker&lt;/em&gt;, it is generally a good idea to use an &lt;em&gt;Exponential Backoff&lt;/em&gt; for testing the health of the dependent connections. Exponential Backoffs will be the topic of my next blog post.&lt;/p&gt;

&lt;p&gt;Stay tuned!&lt;/p&gt;</content><author><name></name></author><summary type="html">The Circuit Breaker pattern is a very useful pattern to improve reliability and availability of a back-end service or micro-service.</summary></entry><entry><title type="html">Welcome to my new website!</title><link href="http://localhost:4000/about/blog/2018/09/05/welcome-to-my-website.html" rel="alternate" type="text/html" title="Welcome to my new website!" /><published>2018-09-05T18:04:44-04:00</published><updated>2018-09-05T18:04:44-04:00</updated><id>http://localhost:4000/about/blog/2018/09/05/welcome-to-my-website</id><content type="html" xml:base="http://localhost:4000/about/blog/2018/09/05/welcome-to-my-website.html">&lt;p&gt;This is my new website. It is built on jekyll and hosted on github pages. It is a simple yet powerful and extensible set up.&lt;/p&gt;

&lt;p&gt;I plan to use it as a way to share some of my personal thoughts and whatever interesting topics (in my field of work or not) I feel good about writing.
You shouldn’t expect scheduled updates. Even though I plan to be dilligent about writing, I won’t just write for the sake of writing, so updates will come as I find interesting things to write about.&lt;/p&gt;

&lt;p&gt;The first series I plan on writing about is about patterns for reliable backend systems (or microservices). Stay tuned!&lt;/p&gt;</content><author><name></name></author><summary type="html">This is my new website. It is built on jekyll and hosted on github pages. It is a simple yet powerful and extensible set up.</summary></entry></feed>